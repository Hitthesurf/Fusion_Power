@article{STAB,
title = {Asymptotic Preserving scheme for strongly anisotropic parabolic equations for arbitrary anisotropy direction},
journal = {Computer Physics Communications},
volume = {185},
number = {12},
pages = {3189-3203},
year = {2014},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2014.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0010465514002999},
author = {Jacek Narski and Maurizio Ottaviani},
keywords = {Anisotropic parabolic equation, Ill-conditioned problem, Singular perturbation model, Limit model, Asymptotic Preserving scheme, Magnetic island},
abstract = {This paper deals with the numerical study of a strongly anisotropic heat equation. The use of standard schemes in this situation leads to poor results, due to high anisotropy. Furthermore, the recently proposed Asymptotic-Preserving method (Lozinski et al., 2012) allows one to perform simulations regardless of the anisotropy strength but its application is limited to the case where the anisotropy direction is given by a field whose lines are all open. In this paper we introduce a new Asymptotic-Preserving method, which overcomes those limitations without any loss of precision or increase in computational costs. The convergence of the method is shown to be independent of the anisotropy parameter 0<ε<1 for fixed coarse Cartesian grids, and for variable anisotropy directions. The context of this work is magnetically confined fusion plasmas.}
}

@article{MMAP,
title = {An asymptotic-preserving method for highly anisotropic elliptic equations based on a Micro–Macro decomposition},
journal = {Journal of Computational Physics},
volume = {231},
number = {7},
pages = {2724-2740},
year = {2012},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2011.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S0021999111006966},
author = {Pierre Degond and Alexei Lozinski and Jacek Narski and Claudia Negulescu},
keywords = {Anisotropic diffusion, Asymptotic preserving scheme, Finite element method},
abstract = {The concern of the present work is the introduction of a very efficient asymptotic preserving scheme for the resolution of highly anisotropic diffusion equations. The characteristic features of this scheme are the uniform convergence with respect to the anisotropy parameter 0<ε≪1, the applicability (on cartesian grids) to cases of non-uniform and non-aligned anisotropy fields b and the simple extension to the case of a non-constant anisotropy intensity 1/ε. The mathematical approach and the numerical scheme are different from those presented in the previous work [P. Degond, F. Deluzet, A. Lozinski, J. Narski, C. Negulescu, Duality-based asymptotic-preserving method for highly anisotropic diffusion equations, Communications in Mathematical Sciences 10 (1) (2012) 1–31] and its considerable advantages are pointed out.}
}

@article{LINE_INT,
title = {An asymptotic preserving method for strongly anisotropic diffusion equations based on field line integration},
journal = {Journal of Computational Physics},
volume = {330},
pages = {735-748},
year = {2017},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2016.10.062},
url = {https://www.sciencedirect.com/science/article/pii/S0021999116305708},
author = {Min Tang and Yihong Wang},
keywords = {Anisotropic diffusion, Asymptotic preserving, Uniform convergence, Field line integration},
abstract = {In magnetized plasma, the magnetic field confines the particles around the field lines. The anisotropy intensity in the viscosity and heat conduction may reach the order of 1012. When the boundary conditions are periodic or Neumann, the strong diffusion leads to an ill-posed limiting problem. To remove the ill-conditionedness in the highly anisotropic diffusion equations, we introduce a simple but very efficient asymptotic preserving reformulation in this paper. The key idea is that, instead of discretizing the Neumann boundary conditions locally, we replace one of the Neumann boundary condition by the integration of the original problem along the field line, the singular 1/ϵ terms can be replaced by O(1) terms after the integration, which yields a well-posed problem. Small modifications to the original code are required and no change of coordinates nor mesh adaptation are needed. Uniform convergence with respect to the anisotropy strength 1/ϵ can be observed numerically and the condition number does not scale with the anisotropy.}
}

@article{AP,
author = {Degond, Pierre and Deluzet, Fabrice and Lozinski, Alexei and Narski, Jacek and Negulescu, Claudia},
year = {2010},
month = {08},
pages = {},
title = {Duality-based Asymptotic-Preserving method for highly anisotropic diffusion equations},
volume = {10},
journal = {Communications in Mathematical Sciences},
doi = {10.4310/CMS.2012.v10.n1.a2}
}

@article{DN,
author = {Deluzet, Fabrice and Narski, Jacek},
title = {A Two Field Iterated Asymptotic-Preserving Method for Highly Anisotropic Elliptic Equations},
year = {2019},
issue_date = {2019},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
volume = {17},
number = {1},
issn = {1540-3459},
url = {https://doi.org/10.1137/17M115205X},
doi = {10.1137/17M115205X},
abstract = {A new two field iterated asymptotic-preserving method is introduced for the numerical resolution of strongly anisotropic elliptic equations. This method does not rely on any integration of the field defining the anisotropy. It rather harnesses an auxiliary variable removing any stiffness from the equation. Compared to precedent realizations using the same approach, the iterated method allows for the resolution of each field independently within an iterative process to converge the two unknowns. This brings advantages in the computational efficiency of the method for large meshes, a better scaling of the matrices condition number with respect to the mesh refinement, as well as the ability to address complex anisotropy topology including closed field lines.},
journal = {Multiscale Model. Simul.},
month = {jan},
pages = {434–459},
numpages = {26},
keywords = {asymptotic-preserving scheme, anisotropic diffusion, iterative method, 65N30}
}

@misc{defelement,
       AUTHOR = {{The DefElement contributors}},
        TITLE = {{D}ef{E}lement: an encyclopedia of finite element definitions},
         YEAR = {{2022}},
 HOWPUBLISHED = {\url{https://defelement.com}},
         NOTE = {[Online; accessed 14-August-2022]}
}

@online {Dragon,
title = {Firedrake},
year = {2022},
URL = {https://firedrakeproject.org/},
note = {(Last Visited 14/08/2022)}
}

@online {FEniCSx,
title = {FEniCSx},
year = {2022},
URL = {https://fenicsproject.org/},
note = {(Last Visited 14/08/2022)}
}

@online {Hub,
title = {Fusion Power Repository},
year = {2022},
URL = {https://github.com/Hitthesurf/Fusion_Power},
note = {(Last Visited 14/08/2022)}
}

@ARTICLE{SciPy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@MISC{Cusp,
  author = "Steven Dalton and Nathan Bell and Luke Olson and Michael Garland",
  title = "Cusp: Generic Parallel Algorithms for Sparse Matrix and Graph Computations",
  year = "2014",
  url = "http://cusplibrary.github.io/",
  note = "Version 0.5.0"
}

@misc{thermal1,
       AUTHOR = {D. Schmid},
        TITLE = {Matrix: Schmid/thermal1},
         YEAR = {2006},
URL = {https://www.cise.ufl.edu/research/sparse/matrices/Schmid/thermal1.html},
         NOTE = {[Online; accessed 14-August-2022]}
}



@misc{thesisGPUPara,
       AUTHOR = {Morgan Görtz},
        TITLE = {Parallelization Of The GMRES
Method},
URL = {https://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=8889583&fileOId=8890947},
         NOTE = {[Online; accessed 14-August-2022]}
}

@inproceedings{SpeedClaim,
author = {Bahi, Jacques M. and Couturier, Rapha\"{e}l and Khodja, Lilia Ziane},
title = {Parallel GMRES Implementation for Solving Sparse Linear Systems on GPU Clusters},
year = {2011},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {In this paper, we propose an efficient parallel implementation of the GMRES method for GPU clusters. This implementation requires us to parallelize the GMRES algorithm between the CPUs of the cluster. Hence, all parallel and intensive computations on local data are performed on GPUs and reduction operations to compute global results are carried out by CPUs. The performances of our parallel GMRES solver are evaluated on test matrices of sizes exceeding 107 rows. They show that solving large and sparse linear systems on a GPU cluster is faster than those performed on its CPU counterpart. It is noticed that a cluster of 12 GPUs is about 8 times faster than a cluster of 12 CPUs and about 5 times faster than a cluster of 24 CPUs.},
booktitle = {Proceedings of the 19th High Performance Computing Symposia},
pages = {12–19},
numpages = {8},
keywords = {sparse linear systems, GPU cluster, GMRES, CUDA},
location = {Boston, Massachusetts},
series = {HPC '11}
}

@misc{CPUSPEC,
        TITLE = {Intel® Core™ i7-11800H Processor},
URL = {https://www.intel.com/content/www/us/en/products/sku/213803/intel-core-i711800h-processor-24m-cache-up-to-4-60-ghz/specifications.html},
         NOTE = {[Online; accessed 26-August-2022]}
}

@misc{GPUSPEC,
        TITLE = {NVIDIA GeForce RTX 3050 Ti Mobile},
URL = {https://www.techpowerup.com/gpu-specs/geforce-rtx-3050-ti-mobile.c3778},
         NOTE = {[Online; accessed 26-August-2022]}
}

@misc{GPUIS,
        TITLE = {GPU support for Firedrake 1605},
URL = {https://github.com/firedrakeproject/firedrake/pull/1605},
         NOTE = {[Online; accessed 26-August-2022]}
}

@misc{CUDA,
        TITLE = {NVIDIA CUDA},
URL = {https://developer.nvidia.com/cuda-toolkit},
         NOTE = {[Online; accessed 26-August-2022]}
}

@misc{Para,
        TITLE = {ParaView},
URL = {https://www.paraview.org/},
         NOTE = {[Online; accessed 26-August-2022]}
}


@misc{Ipyvolume,
        TITLE = {Ipyvolume},
URL = {https://ipyvolume.readthedocs.io/en/latest/},
         NOTE = {[Online; accessed 26-August-2022]}
}

@Article{Matplotlib,
  Author    = {Hunter, J. D.},
  Title     = {Matplotlib: A 2D graphics environment},
  Journal   = {Computing in Science \& Engineering},
  Volume    = {9},
  Number    = {3},
  Pages     = {90--95},
  abstract  = {Matplotlib is a 2D graphics package used for Python for
  application development, interactive scripting, and publication-quality
  image generation across user interfaces and operating systems.},
  publisher = {IEEE COMPUTER SOC},
  doi       = {10.1109/MCSE.2007.55},
  year      = 2007
}


@article{COOFormat,
title = {The Sliced COO Format for Sparse Matrix-Vector Multiplication on CUDA-enabled GPUs},
journal = {Procedia Computer Science},
volume = {9},
pages = {57-66},
year = {2012},
note = {Proceedings of the International Conference on Computational Science, ICCS 2012},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912001287},
author = {Hoang-Vu Dang and Bertil Schmidt},
keywords = {SpMV, CUDA, Fermi},
abstract = {Existing formats for Sparse Matrix-Vector Multiplication (SpMV) on the GPU are outperforming their corresponding implementations on multi-core CPUs. In this paper, we present a new format called Sliced COO (SCOO) and an effcient CUDA implementation to perform SpMV on the GPU. While previous work shows experiments on small to medium-sized sparse matrices, we perform evaluations on large sparse matrices. We compared SCOO performance to existing formats of the NVIDIA Cusp library. Our resutls on a Fermi GPU show that SCOO outperforms the COO and CSR format for all tested matrices and the HYB format for all tested unstructured matrices. Furthermore, comparison to a Sandy-Bridge CPU shows that SCOO on a Fermi GPU outperforms the multi-threaded CSR implementation of the Intel MKL Library on an i7-2700K by a factor between 5.5 and 18.}
}



